{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "GridRL.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "keqTu3koB3E2",
        "colab_type": "code",
        "outputId": "b643901c-7688-4879-b7de-489eb4643712",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "model_dir = '/content/gdrive/My Drive/Winter 2020/DL/Project 2/models/'\n",
        "!cp /content/gdrive/My\\ Drive/Winter\\ 2020/DL/Project\\ 2/data/*.zip .\n",
        "!unzip /content/sudoku.zip\n",
        "!mkdir /content/test\n",
        "!unzip /content/sudoku_test.zip -d /content/test\n",
        "!mv /content/test/sudoku.csv /content/sudoku_test.csv"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Archive:  /content/sudoku.zip\n",
            "  inflating: sudoku.csv              \n",
            "Archive:  /content/sudoku_test.zip\n",
            "  inflating: /content/test/sudoku.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfFdkoy2B3E8",
        "colab_type": "code",
        "outputId": "8baaab76-0487-4d62-f028-c48b0b0eda98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!git clone https://github.com/cloughurd/drl-sudoku.git\n",
        "!mv drl-sudoku/rl/* ."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'drl-sudoku'...\n",
            "remote: Enumerating objects: 250, done.\u001b[K\n",
            "remote: Counting objects: 100% (250/250), done.\u001b[K\n",
            "remote: Compressing objects: 100% (196/196), done.\u001b[K\n",
            "remote: Total 250 (delta 150), reused 106 (delta 46), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (250/250), 380.17 KiB | 4.18 MiB/s, done.\n",
            "Resolving deltas: 100% (150/150), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeLm-_9wB3FA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import json\n",
        "\n",
        "assert torch.cuda.is_available()\n",
        "from IPython.core.ultratb import AutoFormattedTB\n",
        "__ITB__ = AutoFormattedTB(mode = 'Verbose',color_scheme='LightBg', tb_offset = 1)\n",
        "\n",
        "from env.fullgrid import GridEnv\n",
        "from helpers import prepare_batch, learn_dqn, get_action_dqn\n",
        "from qnetwork import QNetwork"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OX3704yIB3FD",
        "colab_type": "code",
        "outputId": "86e20b70-61c8-4298-8ba3-b333bcfecef7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "source": [
        "def dqn_main(num_epochs=50000):\n",
        "    # Hyper parameters\n",
        "    lr = 1e-3\n",
        "    start_training = 1000\n",
        "    gamma = 0.99\n",
        "    batch_size = 32\n",
        "    epsilon = 1\n",
        "    epsilon_decay = .9999\n",
        "    target_update = 1000\n",
        "    learn_frequency = 4\n",
        "\n",
        "    # Init environment\n",
        "    action_size = 9*81\n",
        "    env = GridEnv('/content/sudoku.csv', max_len=5000000)\n",
        "\n",
        "    # Init networks\n",
        "    q_network = QNetwork(18, action_size).cuda()\n",
        "    target_network = QNetwork(18, action_size).cuda()\n",
        "    target_network.load_state_dict(q_network.state_dict())\n",
        "\n",
        "    # Init optimizer\n",
        "    optim = torch.optim.Adam(q_network.parameters(), lr=lr)\n",
        "\n",
        "    # Init replay buffer\n",
        "    memory = []\n",
        "\n",
        "    total_learnings = 0\n",
        "\n",
        "    # Begin main loop\n",
        "    save_freq = 5000\n",
        "    results_dqn = []\n",
        "    losses = []\n",
        "    reward_curves = {}\n",
        "    global_step = 0\n",
        "    loop = tqdm(total=num_epochs, position=0, leave=False)\n",
        "    for epoch in range(num_epochs):\n",
        "        # New puzzle\n",
        "        state, goal = env.reset()\n",
        "        done = False\n",
        "        cum_reward = 0  # Track cumulative reward per episode\n",
        "        rewards = []\n",
        "        pos_count = 0\n",
        "\n",
        "        # Begin episode\n",
        "        while not done and abs(cum_reward) < 16:\n",
        "            # Select e-greedy action\n",
        "            action, epsilon = get_action_dqn(q_network, state, epsilon, epsilon_decay)\n",
        "\n",
        "            # Take step\n",
        "            next_state, reward, done = env.act(state, action, goal)\n",
        "            # env.render()\n",
        "\n",
        "            # Store step in replay buffer\n",
        "            memory.append((state, action, next_state, reward, done))\n",
        "\n",
        "            if reward >= 0:\n",
        "              pos_count += 1\n",
        "            cum_reward += reward\n",
        "            rewards.append(reward)\n",
        "            global_step += 1  # Increment total steps\n",
        "            state = next_state  # Set current state\n",
        "\n",
        "            # If time to train\n",
        "            if global_step > start_training and global_step % learn_frequency == 0:\n",
        "                total_learnings += 1\n",
        "\n",
        "                # Sample batch\n",
        "                batch = prepare_batch(memory, batch_size)\n",
        "\n",
        "                # Train\n",
        "                loss = learn_dqn(batch, optim, q_network, target_network, gamma, global_step, target_update)\n",
        "                losses.append((global_step, loss))\n",
        "\n",
        "        # Print results at end of episode\n",
        "        results_dqn.append(cum_reward)\n",
        "        reward_curves[epoch] = rewards\n",
        "        loop.update(1)\n",
        "        loop.set_description('Episodes: {} Reward: {} Epsilon: {:.4f} Positive Reward Count: {}'.format(epoch, cum_reward, epsilon, pos_count))\n",
        "        \n",
        "        if epoch+1 % save_freq == 0:\n",
        "            torch.save(q_network.state_dict(), model_dir + f'rl-{epoch}.mod')\n",
        "            json.dump({'rewards': reward_curves, 'loss': losses},\n",
        "                      open(model_dir + f'rl-results-{epoch}.json', 'w'))\n",
        "\n",
        "    print(total_learnings)\n",
        "    return results_dqn\n",
        "\n",
        "results_dqn = dqn_main()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episodes: 3999 Reward: -16 Epsilon: 0.0785 Positive Reward Count: 0:   8%|▊         | 4000/50000 [44:25<8:52:03,  1.44it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-d4d7bd6203f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults_dqn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mresults_dqn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdqn_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-d4d7bd6203f3>\u001b[0m in \u001b[0;36mdqn_main\u001b[0;34m(num_epochs)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_update\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m                 \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/helpers.py\u001b[0m in \u001b[0;36mlearn_dqn\u001b[0;34m(batch, optim, q_network, target_network, gamma, global_step, target_update, device)\u001b[0m\n\u001b[1;32m     79\u001b[0m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m   \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtarget_update\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlOxsXZTB3FI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(results_dqn)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHSE63VoB3FM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}