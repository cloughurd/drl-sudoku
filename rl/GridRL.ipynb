{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "GridRL.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "keqTu3koB3E2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "5e865076-44a5-4938-c350-2014cbeae94a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "model_dir = '/content/gdrive/My Drive/Winter 2020/DL/Project 2/models/'\n",
        "!cp /content/gdrive/My\\ Drive/Winter\\ 2020/DL/Project\\ 2/data/*.zip .\n",
        "!unzip /content/sudoku.zip\n",
        "!mkdir /content/test\n",
        "!unzip /content/sudoku_test.zip -d /content/test\n",
        "!mv /content/test/sudoku.csv /content/sudoku_test.csv"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Archive:  /content/sudoku.zip\n",
            "  inflating: sudoku.csv              \n",
            "Archive:  /content/sudoku_test.zip\n",
            "  inflating: /content/test/sudoku.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfFdkoy2B3E8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "bfe4ba52-b896-4758-e8a7-987f1b3500c2"
      },
      "source": [
        "!git clone https://github.com/cloughurd/drl-sudoku.git\n",
        "!mv drl-sudoku/rl/* ."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'drl-sudoku'...\n",
            "remote: Enumerating objects: 231, done.\u001b[K\n",
            "remote: Counting objects: 100% (231/231), done.\u001b[K\n",
            "remote: Compressing objects: 100% (186/186), done.\u001b[K\n",
            "remote: Total 231 (delta 137), reused 92 (delta 37), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (231/231), 354.00 KiB | 3.19 MiB/s, done.\n",
            "Resolving deltas: 100% (137/137), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeLm-_9wB3FA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import json\n",
        "\n",
        "assert torch.cuda.is_available()\n",
        "from IPython.core.ultratb import AutoFormattedTB\n",
        "__ITB__ = AutoFormattedTB(mode = 'Verbose',color_scheme='LightBg', tb_offset = 1)\n",
        "\n",
        "from env.fullgrid import GridEnv\n",
        "from helpers import prepare_batch, learn_dqn, get_action_dqn\n",
        "from qnetwork import QNetwork"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OX3704yIB3FD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "57f36ef3-a12f-48a7-fdfd-40816cba5638"
      },
      "source": [
        "def dqn_main(num_epochs=50000):\n",
        "    # Hyper parameters\n",
        "    lr = 1e-3\n",
        "    start_training = 1000\n",
        "    gamma = 0.99\n",
        "    batch_size = 32\n",
        "    epsilon = 1\n",
        "    epsilon_decay = .9999\n",
        "    target_update = 1000\n",
        "    learn_frequency = 4\n",
        "\n",
        "    # Init environment\n",
        "    action_size = 9*81\n",
        "    env = GridEnv('/content/sudoku.csv', max_len=5000000)\n",
        "\n",
        "    # Init networks\n",
        "    q_network = QNetwork(18, action_size).cuda()\n",
        "    target_network = QNetwork(18, action_size).cuda()\n",
        "    target_network.load_state_dict(q_network.state_dict())\n",
        "\n",
        "    # Init optimizer\n",
        "    optim = torch.optim.Adam(q_network.parameters(), lr=lr)\n",
        "\n",
        "    # Init replay buffer\n",
        "    memory = []\n",
        "\n",
        "    total_learnings = 0\n",
        "\n",
        "    # Begin main loop\n",
        "    save_freq = 5000\n",
        "    results_dqn = []\n",
        "    losses = []\n",
        "    reward_curves = {}\n",
        "    global_step = 0\n",
        "    loop = tqdm(total=num_epochs, position=0, leave=False)\n",
        "    for epoch in range(num_epochs):\n",
        "        # New puzzle\n",
        "        state, goal = env.reset()\n",
        "        done = False\n",
        "        cum_reward = 0  # Track cumulative reward per episode\n",
        "        rewards = []\n",
        "        pos_count = 0\n",
        "\n",
        "        # Begin episode\n",
        "        while not done and abs(cum_reward) < 20:\n",
        "            # Select e-greedy action\n",
        "            action, epsilon = get_action_dqn(q_network, state, epsilon, epsilon_decay)\n",
        "\n",
        "            # Take step\n",
        "            next_state, reward, done = env.act(state, action, goal)\n",
        "            # env.render()\n",
        "\n",
        "            # Store step in replay buffer\n",
        "            memory.append((state, action, next_state, reward, done))\n",
        "\n",
        "            if reward >= 0:\n",
        "              pos_count += 1\n",
        "            cum_reward += reward\n",
        "            rewards.append(reward)\n",
        "            global_step += 1  # Increment total steps\n",
        "            state = next_state  # Set current state\n",
        "\n",
        "            # If time to train\n",
        "            if global_step > start_training and global_step % learn_frequency == 0:\n",
        "                total_learnings += 1\n",
        "\n",
        "                # Sample batch\n",
        "                batch = prepare_batch(memory, batch_size)\n",
        "\n",
        "                # Train\n",
        "                loss = learn_dqn(batch, optim, q_network, target_network, gamma, global_step, target_update)\n",
        "                losses.append((global_step, loss))\n",
        "\n",
        "        # Print results at end of episode\n",
        "        results_dqn.append(cum_reward)\n",
        "        reward_curves[epoch] = rewards\n",
        "        loop.update(1)\n",
        "        loop.set_description('Episodes: {} Reward: {} Epsilon: {:.4f} Positive Reward Count: {}'.format(epoch, cum_reward, epsilon, pos_count))\n",
        "        \n",
        "        if epoch % save_freq == 0:\n",
        "            torch.save(q_network.state_dict(), model_dir + f'rl-{epoch}.mod')\n",
        "            json.dump({'rewards': reward_curves, 'loss': losses},\n",
        "                      open(model_dir + f'rl-results-{epoch}.json', 'w'))\n",
        "\n",
        "    print(total_learnings)\n",
        "    return results_dqn\n",
        "\n",
        "results_dqn = dqn_main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episodes: 18911 Reward: -8 Epsilon: 0.0000 Positive Reward Count: 2:  38%|███▊      | 18912/50000 [9:17:30<11:03:06,  1.28s/it] "
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlOxsXZTB3FI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(results_dqn)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHSE63VoB3FM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}